{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8863606-0d30-44ef-8994-18a7dacbf94e",
   "metadata": {},
   "source": [
    "## CODE TO GENERATE word_dict.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a8df95a-b692-44e2-a2fb-fa4c821c30b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diccionario creado con 31005 palabras.\n",
      "Guardado en: ./mind_small\\utils\\word_dict.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "# --- Paths ---\n",
    "base_path = \"./mind_small\"\n",
    "train_news_path = os.path.join(base_path, \"MINDsmall_train\", \"news.tsv\")\n",
    "\n",
    "utils_path = os.path.join(base_path, \"utils\")\n",
    "os.makedirs(utils_path, exist_ok=True)\n",
    "\n",
    "output_file = os.path.join(utils_path, \"word_dict.pkl\")\n",
    "\n",
    "# --- Clean function ---\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9 ]+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "# --- Read news.tsv ---\n",
    "train_df = pd.read_csv(train_news_path, sep=\"\\t\", header=None,\n",
    "                       names=[\"id\", \"category\", \"subcategory\", \"title\", \"abstract\", \"url\", \"ents_title\", \"ents_abs\"])\n",
    "\n",
    "# --- Mix titles ---\n",
    "titles = list(train_df[\"title\"].astype(str))\n",
    "\n",
    "# --- Clean and tokenize ---\n",
    "tokens = []\n",
    "for t in titles:\n",
    "    t = clean_text(t)\n",
    "    tokens.extend(t.split())\n",
    "\n",
    "# --- Vocabulary and frequency ---\n",
    "counter = Counter(tokens)\n",
    "\n",
    "# --- Make the dict word → index ---\n",
    "# Special tokens\n",
    "word_dict = {\n",
    "    \"<PAD>\": 0,\n",
    "    \"<UNK>\": 1\n",
    "}\n",
    "\n",
    "# Add frequent words\n",
    "for i, (word, freq) in enumerate(counter.items(), start=2):\n",
    "    word_dict[word] = i\n",
    "\n",
    "# --- Save ---\n",
    "with open(output_file, \"wb\") as f:\n",
    "    pickle.dump(word_dict, f)\n",
    "\n",
    "print(f\"Diccionario creado con {len(word_dict)} palabras.\")\n",
    "print(f\"Guardado en: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c315cc8e-90c1-4bed-8a3d-94749f699c3f",
   "metadata": {},
   "source": [
    "## CODE TO GENERATE word_dict_all.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61491f45-c5b3-4049-bb3d-0d6b46c7b650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diccionario creado con 54914 palabras.\n",
      "Guardado en: ./mind_small\\utils\\word_dict_all.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "# --- Paths ---\n",
    "base_path = \"./mind_small\"\n",
    "train_news_path = os.path.join(base_path, \"MINDsmall_train\", \"news.tsv\")\n",
    "\n",
    "utils_path = os.path.join(base_path, \"utils\")\n",
    "os.makedirs(utils_path, exist_ok=True)\n",
    "\n",
    "output_file = os.path.join(utils_path, \"word_dict_all.pkl\")\n",
    "\n",
    "# --- Clean function ---\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9 ]+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "# --- Read news.tsv ---\n",
    "train_df = pd.read_csv(train_news_path, sep=\"\\t\", header=None,\n",
    "                       names=[\"id\", \"category\", \"subcategory\", \"title\", \"abstract\", \"url\", \"ents_title\", \"ents_abs\"])\n",
    "\n",
    "# --- Mix titles ---\n",
    "titles = list(train_df[\"title\"].astype(str))\n",
    "abstracts = list(train_df[\"abstract\"].astype(str))\n",
    "\n",
    "# --- Clean and tokenize ---\n",
    "tokens = []\n",
    "for t in titles:\n",
    "    t = clean_text(t)\n",
    "    tokens.extend(t.split())\n",
    "for a in abstracts:\n",
    "    a = clean_text(a)\n",
    "    tokens.extend(a.split())\n",
    "\n",
    "# --- Vocabulary and frequency ---\n",
    "counter = Counter(tokens)\n",
    "\n",
    "# --- Make the dict word → index ---\n",
    "# Special tokens\n",
    "word_dict_all = {\n",
    "    \"<PAD>\": 0,\n",
    "    \"<UNK>\": 1\n",
    "}\n",
    "\n",
    "# Add frequent words\n",
    "for i, (word, freq) in enumerate(counter.items(), start=2):\n",
    "    word_dict_all[word] = i\n",
    "\n",
    "# --- Save ---\n",
    "with open(output_file, \"wb\") as f:\n",
    "    pickle.dump(word_dict_all, f)\n",
    "\n",
    "print(f\"Diccionario creado con {len(word_dict_all)} palabras.\")\n",
    "print(f\"Guardado en: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5e9c3f-b766-4c83-8eb9-00e82d4c2421",
   "metadata": {},
   "source": [
    "## CODE TO GENERATE vert_dict.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ccc55ca-80af-4fa0-a0ff-30aa11a5322c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diccionario de categorías creado y guardado en: ./mind_small\\utils\\vert_dict.pkl\n",
      "Categorías encontradas: {'autos': 0, 'entertainment': 1, 'finance': 2, 'foodanddrink': 3, 'health': 4, 'kids': 5, 'lifestyle': 6, 'middleeast': 7, 'movies': 8, 'music': 9, 'news': 10, 'northamerica': 11, 'sports': 12, 'travel': 13, 'tv': 14, 'video': 15, 'weather': 16}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Ruta donde tienes el dataset\n",
    "data_path = \"./mind_small\"\n",
    "utils_path = os.path.join(data_path, \"utils\")\n",
    "os.makedirs(utils_path, exist_ok=True)\n",
    "\n",
    "# Fichero de noticias\n",
    "news_file = os.path.join(data_path, \"MINDsmall_train/news.tsv\")\n",
    "\n",
    "# Conjunto donde almacenaremos todas las categorías únicas\n",
    "vert_set = set()\n",
    "\n",
    "# Leer el archivo de noticias y extraer la columna \"category\"\n",
    "with open(news_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) < 2:\n",
    "            continue\n",
    "        vert = parts[1]   # segunda columna → categoría\n",
    "        vert_set.add(vert)\n",
    "\n",
    "# Crear diccionario: categoría → índice\n",
    "vert_dict = {vert: idx for idx, vert in enumerate(sorted(vert_set))}\n",
    "\n",
    "# Guardar el diccionario como PKL\n",
    "vert_dict_path = os.path.join(utils_path, \"vert_dict.pkl\")\n",
    "with open(vert_dict_path, \"wb\") as f:\n",
    "    pickle.dump(vert_dict, f)\n",
    "\n",
    "print(\"Diccionario de categorías creado y guardado en:\", vert_dict_path)\n",
    "print(\"Categorías encontradas:\", vert_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cfa6d3-b639-471a-b46d-90329e7275cf",
   "metadata": {},
   "source": [
    "## CODE TO GENERATE subvert_dict.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfdf87b8-01ce-4ac5-8998-3fd5a617c158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diccionario de subcategorías creado y guardado en: ./mind_small\\utils\\subvert_dict.pkl\n",
      "Subcategorías encontradas: {'ads-latingrammys': 0, 'ads-lung-health': 1, 'advice': 2, 'animals': 3, 'autosbuying': 4, 'autoscartech': 5, 'autosclassics': 6, 'autoscompact': 7, 'autosenthusiasts': 8, 'autoshybrids': 9, 'autoslosangeles': 10, 'autosluxury': 11, 'autosmidsize': 12, 'autosmotorcycles': 13, 'autosnews': 14, 'autosownership': 15, 'autospassenger': 16, 'autosresearch': 17, 'autosresearchguides': 18, 'autosreview': 19, 'autossema': 20, 'autossports': 21, 'autossuvs': 22, 'autostokyo': 23, 'autostrucks': 24, 'autosvans': 25, 'autosvideonew': 26, 'autosvideos': 27, 'awards': 28, 'awardstyle': 29, 'baseball': 30, 'baseball_mlb': 31, 'baseball_mlb_videos': 32, 'basketball_nba': 33, 'basketball_nba_videos': 34, 'basketball_ncaa': 35, 'basketball_ncaa_videos': 36, 'basketball_wnba': 37, 'beverages': 38, 'boxing': 39, 'boxing-mma': 40, 'cardio': 41, 'career-news': 42, 'causes': 43, 'causes-animals': 44, 'causes-disaster-relief': 45, 'causes-environment': 46, 'causes-food-insecurity': 47, 'causes-green-living': 48, 'causes-military-appreciation': 49, 'causes-poverty': 50, 'celebhub': 51, 'celebrity': 52, 'celebritynews': 53, 'cma-awards': 54, 'cocktails': 55, 'comedy': 56, 'company-news': 57, 'cooking': 58, 'cookingschool': 59, 'downtime': 60, 'elections-2020-us': 61, 'empowering-the-planet': 62, 'entertainment-books': 63, 'entertainment-celebrity': 64, 'entertainmentmusic': 65, 'entertainmenttv': 66, 'factcheck': 67, 'finance-auto-insurance': 68, 'finance-billstopay': 69, 'finance-career': 70, 'finance-career-education': 71, 'finance-companies': 72, 'finance-credit': 73, 'finance-education': 74, 'finance-healthcare': 75, 'finance-home-loans': 76, 'finance-homesandpropertysection': 77, 'finance-insidetheticker': 78, 'finance-insurance': 79, 'finance-mutual-funds': 80, 'finance-real-estate': 81, 'finance-retirement': 82, 'finance-savemoney': 83, 'finance-saving-investing': 84, 'finance-small-business': 85, 'finance-startinvesting': 86, 'finance-taxes': 87, 'finance-technology': 88, 'finance-top-stocks': 89, 'finance-video': 90, 'financenews': 91, 'fitness': 92, 'foodanddrink': 93, 'foodnews': 94, 'foodrecipes': 95, 'foodtips': 96, 'football_ncaa': 97, 'football_ncaa_videos': 98, 'football_nfl': 99, 'football_nfl_videos': 100, 'fun': 101, 'games': 102, 'gaming': 103, 'golf': 104, 'golfvideos': 105, 'halloween': 106, 'health-cancer': 107, 'health-news': 108, 'healthagingwell': 109, 'healthnews': 110, 'healthyliving': 111, 'holidays': 112, 'hollywood': 113, 'humor': 114, 'icehockey_nhl': 115, 'indepth': 116, 'internationaltravel': 117, 'lifestyle': 118, 'lifestyle-news-feature': 119, 'lifestyle-wedding': 120, 'lifestylebeauty': 121, 'lifestylebuzz': 122, 'lifestylecareer': 123, 'lifestylecelebstyle': 124, 'lifestylecleaningandorganizing': 125, 'lifestyledecor': 126, 'lifestyledidyouknow': 127, 'lifestylediy': 128, 'lifestylefamily': 129, 'lifestylefamilyandrelationships': 130, 'lifestylefashion': 131, 'lifestylehomeandgarden': 132, 'lifestylehoroscope': 133, 'lifestylehoroscopefish': 134, 'lifestylelovesex': 135, 'lifestylemarriage': 136, 'lifestylemindandsoul': 137, 'lifestyleparenting': 138, 'lifestylepets': 139, 'lifestylepetsanimals': 140, 'lifestylerelationships': 141, 'lifestyleroyals': 142, 'lifestyleshopping': 143, 'lifestyleshoppinghomegarden': 144, 'lifestylesmartliving': 145, 'lifestylestyle': 146, 'lifestylevideo': 147, 'lifestyleweddings': 148, 'lifestylewhatshot': 149, 'markets': 150, 'medical': 151, 'mentalhealth': 152, 'middleeast-top-stories': 153, 'mma': 154, 'mmaufc': 155, 'more_sports': 156, 'movienews': 157, 'movies-awards': 158, 'movies-celebrity': 159, 'movies-gallery': 160, 'movies-oscars': 161, 'movievideo': 162, 'music-awards': 163, 'music-celebrity': 164, 'music-gallery': 165, 'music-grammys': 166, 'music-reviews': 167, 'musicnews': 168, 'musicvideos': 169, 'narendramodi_opinion': 170, 'news': 171, 'newsbusiness': 172, 'newscrime': 173, 'newselection2020': 174, 'newsfactcheck': 175, 'newsgoodnews': 176, 'newsnational': 177, 'newsoffbeat': 178, 'newsopinion': 179, 'newsother': 180, 'newsphotos': 181, 'newspolitics': 182, 'newsrealestate': 183, 'newsscience': 184, 'newsscienceandtechnology': 185, 'newstrends': 186, 'newstvmedia': 187, 'newsus': 188, 'newsvideo': 189, 'newsweather': 190, 'newsworld': 191, 'newsworldpolitics': 192, 'northamerica-video': 193, 'nutrition': 194, 'olympics-videos': 195, 'othersports': 196, 'outdoors': 197, 'people-places': 198, 'peopleandplaces': 199, 'personalfinance': 200, 'photos': 201, 'popculture': 202, 'pregnancyparenting': 203, 'quickandeasy': 204, 'racing': 205, 'recipes': 206, 'relationships': 207, 'restaurantsandnews': 208, 'retirement': 209, 'reviews': 210, 'science': 211, 'seasonal': 212, 'shop-all': 213, 'shop-apparel': 214, 'shop-books-movies-tv': 215, 'shop-holidays': 216, 'shop-home-goods': 217, 'soccer': 218, 'soccer_bund': 219, 'soccer_epl': 220, 'soccer_mls': 221, 'spendingandborrowing': 222, 'sports': 223, 'sports_news': 224, 'strength': 225, 'technologyinvesting': 226, 'tennis': 227, 'tennis_intl': 228, 'tipsandtricks': 229, 'topnews': 230, 'travel': 231, 'travel-adventure-travel': 232, 'travel-points-rewards': 233, 'travel-videos': 234, 'travelarticle': 235, 'travelnews': 236, 'traveltips': 237, 'traveltripideas': 238, 'traveltrivia': 239, 'tunedin': 240, 'tv': 241, 'tv-celebrity': 242, 'tv-gallery': 243, 'tv-golden-globes': 244, 'tv-golden-globes-video': 245, 'tv-recaps': 246, 'tv-reviews': 247, 'tvnews': 248, 'tvvideos': 249, 'ustravel': 250, 'video': 251, 'videos': 252, 'viral': 253, 'voices': 254, 'watch': 255, 'weatherfullscreenmaps': 256, 'weathertopstories': 257, 'weight-loss': 258, 'weightloss': 259, 'wellness': 260, 'wines': 261, 'wonder': 262, 'yearinoffbeatgoodnews': 263}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Ruta donde tienes el dataset\n",
    "data_path = \"./mind_small\"\n",
    "utils_path = os.path.join(data_path, \"utils\")\n",
    "os.makedirs(utils_path, exist_ok=True)\n",
    "\n",
    "# Fichero de noticias (el de TRAIN)\n",
    "news_file = os.path.join(data_path, \"MINDsmall_train/news.tsv\")\n",
    "\n",
    "# Conjunto donde almacenaremos todas las SUBcategorías únicas\n",
    "subvert_set = set()\n",
    "\n",
    "# Leer el archivo de noticias y extraer la columna \"subategory\"\n",
    "with open(news_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) < 3:\n",
    "            continue\n",
    "        subvert = parts[2]   # tercera columna → subcategoría\n",
    "        subvert_set.add(subvert)\n",
    "\n",
    "# Crear diccionario: subcategoría → índice\n",
    "subvert_dict = {subvert: idx for idx, subvert in enumerate(sorted(subvert_set))}\n",
    "\n",
    "# Guardar el diccionario como PKL\n",
    "subvert_dict_path = os.path.join(utils_path, \"subvert_dict.pkl\")\n",
    "with open(subvert_dict_path, \"wb\") as f:\n",
    "    pickle.dump(subvert_dict, f)\n",
    "\n",
    "print(\"Diccionario de subcategorías creado y guardado en:\", subvert_dict_path)\n",
    "print(\"Subcategorías encontradas:\", subvert_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b194e2b0-fc93-43cd-9223-e23cce3c0287",
   "metadata": {},
   "source": [
    "## CODE TO GENERATE uid2index.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ee962e6-90a6-413b-9bb6-7f021603e731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo usuarios de train...\n",
      "Usuarios únicos encontrados: 50000\n",
      "uid2index.pkl guardado en: ./mind_small\\utils\\uid2index.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# ------------------------------\n",
    "# Rutas\n",
    "# ------------------------------\n",
    "data_path = \"./mind_small\"\n",
    "utils_path = os.path.join(data_path, \"utils\")\n",
    "os.makedirs(utils_path, exist_ok=True)\n",
    "\n",
    "train_behaviors = os.path.join(data_path, \"MINDsmall_train\", \"behaviors.tsv\")\n",
    "\n",
    "output_file = os.path.join(utils_path, \"uid2index.pkl\")\n",
    "\n",
    "# ------------------------------\n",
    "# EXTRAER TODOS LOS USER IDS\n",
    "# ------------------------------\n",
    "user_ids = set()\n",
    "\n",
    "def read_users(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) > 1:\n",
    "                user_ids.add(parts[1])\n",
    "\n",
    "print(\"Leyendo usuarios de train...\")\n",
    "read_users(train_behaviors)\n",
    "\n",
    "print(f\"Usuarios únicos encontrados: {len(user_ids)}\")\n",
    "\n",
    "# ------------------------------\n",
    "# ASIGNAR ÍNDICES A CADA USUARIO\n",
    "# ------------------------------\n",
    "uid2index = {uid: idx for idx, uid in enumerate(sorted(user_ids))}\n",
    "\n",
    "# ------------------------------\n",
    "# GUARDAR ARCHIVO\n",
    "# ------------------------------\n",
    "with open(output_file, \"wb\") as f:\n",
    "    pickle.dump(uid2index, f)\n",
    "\n",
    "print(f\"uid2index.pkl guardado en: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8970c78-4bf0-4203-88c2-ad29944e1231",
   "metadata": {},
   "source": [
    "## CODE TO GENERATE embedding.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1998aae-ab53-428d-86ce-f711fbe6513a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del vocabulario: 31005\n",
      "Cargando GloVe, esto puede tardar un poco...\n",
      "Dimensión de los embeddings: 300\n",
      "Embeddings asignados correctamente.\n",
      "embedding.npy guardado en: ./mind_small\\utils\\embedding.npy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# --------------------------\n",
    "# Rutas\n",
    "# --------------------------\n",
    "data_path = \"./mind_small\"\n",
    "utils_path = os.path.join(data_path, \"utils\")\n",
    "\n",
    "word_dict_path = os.path.join(utils_path, \"word_dict.pkl\")\n",
    "embedding_output = os.path.join(utils_path, \"embedding.npy\")\n",
    "\n",
    "# Ruta del fichero GloVe descargado\n",
    "glove_path = \"./mind_small/utils/glove.6B.300d.txt\"\n",
    "\n",
    "# --------------------------\n",
    "# Cargar diccionario\n",
    "# --------------------------\n",
    "with open(word_dict_path, \"rb\") as f:\n",
    "    word_dict = pickle.load(f)\n",
    "\n",
    "vocab_size = len(word_dict)\n",
    "print(\"Tamaño del vocabulario:\", vocab_size)\n",
    "\n",
    "# --------------------------\n",
    "# Cargar embeddings GloVe\n",
    "# --------------------------\n",
    "print(\"Cargando GloVe, esto puede tardar un poco...\")\n",
    "\n",
    "glove_embeddings = {}\n",
    "with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        word = parts[0]\n",
    "        vector = np.asarray(parts[1:], dtype=\"float32\")\n",
    "        glove_embeddings[word] = vector\n",
    "\n",
    "embedding_dim = len(vector)\n",
    "print(\"Dimensión de los embeddings:\", embedding_dim)\n",
    "\n",
    "# --------------------------\n",
    "# Crear matriz de embeddings\n",
    "# --------------------------\n",
    "embedding_matrix = np.random.normal(\n",
    "    loc=0.0, scale=0.1, size=(vocab_size, embedding_dim)\n",
    ")\n",
    "\n",
    "for word, idx in word_dict.items():\n",
    "    if word in glove_embeddings:\n",
    "        embedding_matrix[idx] = glove_embeddings[word]\n",
    "\n",
    "print(\"Embeddings asignados correctamente.\")\n",
    "\n",
    "# --------------------------\n",
    "# Guardar matriz\n",
    "# --------------------------\n",
    "np.save(embedding_output, embedding_matrix)\n",
    "\n",
    "print(f\"embedding.npy guardado en: {embedding_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9274b63e-289a-48a9-ad46-a74a871ad6bf",
   "metadata": {},
   "source": [
    "## CODE TO GENERATE embedding_all.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "587ca036-3e58-413e-8488-32b8f8fe1dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del vocabulario: 54914\n",
      "Cargando GloVe, esto puede tardar un poco...\n",
      "Dimensión de los embeddings: 300\n",
      "Embeddings asignados correctamente.\n",
      "embedding.npy guardado en: ./mind_small\\utils\\embedding_all.npy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# --------------------------\n",
    "# Rutas\n",
    "# --------------------------\n",
    "data_path = \"./mind_small\"\n",
    "utils_path = os.path.join(data_path, \"utils\")\n",
    "\n",
    "word_dict_path = os.path.join(utils_path, \"word_dict_all.pkl\")\n",
    "embedding_output = os.path.join(utils_path, \"embedding_all.npy\")\n",
    "\n",
    "# Ruta del fichero GloVe descargado\n",
    "glove_path = \"./mind_small/utils/glove.6B.300d.txt\"\n",
    "\n",
    "# --------------------------\n",
    "# Cargar diccionario\n",
    "# --------------------------\n",
    "with open(word_dict_path, \"rb\") as f:\n",
    "    word_dict = pickle.load(f)\n",
    "\n",
    "vocab_size = len(word_dict)\n",
    "print(\"Tamaño del vocabulario:\", vocab_size)\n",
    "\n",
    "# --------------------------\n",
    "# Cargar embeddings GloVe\n",
    "# --------------------------\n",
    "print(\"Cargando GloVe, esto puede tardar un poco...\")\n",
    "\n",
    "glove_embeddings = {}\n",
    "with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        word = parts[0]\n",
    "        vector = np.asarray(parts[1:], dtype=\"float32\")\n",
    "        glove_embeddings[word] = vector\n",
    "\n",
    "embedding_dim = len(vector)\n",
    "print(\"Dimensión de los embeddings:\", embedding_dim)\n",
    "\n",
    "# --------------------------\n",
    "# Crear matriz de embeddings\n",
    "# --------------------------\n",
    "embedding_matrix = np.random.normal(\n",
    "    loc=0.0, scale=0.1, size=(vocab_size, embedding_dim)\n",
    ")\n",
    "\n",
    "for word, idx in word_dict.items():\n",
    "    if word in glove_embeddings:\n",
    "        embedding_matrix[idx] = glove_embeddings[word]\n",
    "\n",
    "print(\"Embeddings asignados correctamente.\")\n",
    "\n",
    "# --------------------------\n",
    "# Guardar matriz\n",
    "# --------------------------\n",
    "np.save(embedding_output, embedding_matrix)\n",
    "\n",
    "print(f\"embedding.npy guardado en: {embedding_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3bae97-7ee1-4890-8833-cfafe6e774a0",
   "metadata": {},
   "source": [
    "## CODE TO GENERATE lstur.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47d6dcb5-2bc6-45e2-b873-438734e78ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo lstur.yaml creado en: ./mind_small\\utils\\lstur.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "data_path = \"./mind_small\"\n",
    "utils_path = os.path.join(data_path, \"utils\")\n",
    "os.makedirs(utils_path, exist_ok=True)\n",
    "yaml_file = os.path.join(utils_path, \"lstur.yaml\")\n",
    "\n",
    "config = {\n",
    "    \"data\": {\n",
    "        \"title_size\": 30,\n",
    "        \"his_size\": 50,\n",
    "        \"data_format\": \"news\",\n",
    "        \"npratio\": 4\n",
    "    },\n",
    "    \"info\": {\n",
    "        \"metrics\": [\"group_auc\", \"mean_mrr\", \"ndcg@5;10\"],\n",
    "        \"show_step\": 100000\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"attention_hidden_dim\": 200,\n",
    "        \"word_emb_dim\": 300,\n",
    "        \"dropout\": 0.2,\n",
    "        \"filter_num\": 400,\n",
    "        \"window_size\": 3,\n",
    "        \"cnn_activation\": \"relu\",\n",
    "        \"gru_unit\": 400,\n",
    "        \"type\": \"ini\",\n",
    "        \"model_type\": \"lstur\"\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 4,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"loss\": \"cross_entropy_loss\",\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"support_quick_scoring\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(yaml_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"Archivo lstur.yaml creado en: {yaml_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0a8f7f-ed35-4111-a12b-5344cf7591e7",
   "metadata": {},
   "source": [
    "## CODE TO GENERATE nrms.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae55470d-c136-4db0-8180-6279d34390f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo nrms.yaml creado en: ./mind_small\\utils\\nrms.yaml\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "data_path = \"./mind_small\"\n",
    "utils_path = os.path.join(data_path, \"utils\")\n",
    "os.makedirs(utils_path, exist_ok=True)\n",
    "yaml_file = os.path.join(utils_path, \"nrms.yaml\")\n",
    "\n",
    "# Definir la estructura del YAML para NRMS\n",
    "config = {\n",
    "    \"data\": {\n",
    "        \"title_size\": 30,\n",
    "        \"his_size\": 50,\n",
    "        \"data_format\": \"news\",\n",
    "        \"npratio\": 4,\n",
    "    },\n",
    "    \"info\": {\n",
    "        \"metrics\": [\"group_auc\", \"mean_mrr\", \"ndcg@5;10\"],\n",
    "        \"show_step\": 100000,\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"attention_hidden_dim\": 200,\n",
    "        \"word_emb_dim\": 300,\n",
    "        \"dropout\": 0.2,\n",
    "        \"head_num\": 20,\n",
    "        \"head_dim\": 20,\n",
    "        \"model_type\": \"nrms\",\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 4,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"loss\": \"cross_entropy_loss\",\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"support_quick_scoring\": True,\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(yaml_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"Archivo nrms.yaml creado en: {yaml_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f324c485-c9f6-42ac-bf23-56e59ad43196",
   "metadata": {},
   "source": [
    "## CODE TO GENERATE naml.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68ef2b3f-8468-443e-be53-8c172279c811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichero YAML generado en: ./mind_small/utils/naml.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "# Ruta donde se guardará el YAML\n",
    "output_path = \"./mind_small/utils/naml.yaml\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "# Configuración que queremos guardar\n",
    "config = {\n",
    "    \"data\": {\n",
    "        \"title_size\": 30,\n",
    "        \"body_size\": 50,\n",
    "        \"his_size\": 50,\n",
    "        \"vert_num\": 17,\n",
    "        \"subvert_num\": 264,\n",
    "        \"data_format\": \"naml\",\n",
    "        \"npratio\": 4\n",
    "    },\n",
    "    \"info\": {\n",
    "        \"metrics\": [\"group_auc\", \"mean_mrr\", \"ndcg@5;10\"],\n",
    "        \"show_step\": 100000\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"attention_hidden_dim\": 200,\n",
    "        \"word_emb_dim\": 300,\n",
    "        \"vert_emb_dim\": 100,\n",
    "        \"subvert_emb_dim\": 100,\n",
    "        \"dropout\": 0.2,\n",
    "        \"filter_num\": 400,\n",
    "        \"window_size\": 3,\n",
    "        \"cnn_activation\": \"relu\",\n",
    "        \"model_type\": \"naml\",\n",
    "        \"dense_activation\": \"relu\"\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 4,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"loss\": \"cross_entropy_loss\",\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"support_quick_scoring\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Guardar en YAML\n",
    "with open(output_path, \"w\") as f:\n",
    "    yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"Fichero YAML generado en: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94276de-6186-4ad6-8455-46be8b5cf7b5",
   "metadata": {},
   "source": [
    "## CODE TO GENERATE npa.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "035045a4-9d50-4e73-bc43-23f6236c26f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichero YAML generado en: ./mind_small/utils/npa.yaml\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "# Ruta donde se guardará el YAML\n",
    "output_path = \"./mind_small/utils/npa.yaml\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "# Diccionario con la estructura del YAML\n",
    "config = {\n",
    "    \"data\": {\n",
    "        \"title_size\": 10,\n",
    "        \"his_size\": 50,\n",
    "        \"data_format\": \"news\",\n",
    "        \"npratio\": 4\n",
    "    },\n",
    "    \"info\": {\n",
    "        \"metrics\": [\"group_auc\", \"mean_mrr\", \"ndcg@5;10\"],\n",
    "        \"show_step\": 100000\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"attention_hidden_dim\": 200,\n",
    "        \"word_emb_dim\": 300,\n",
    "        \"user_emb_dim\": 100,\n",
    "        \"dropout\": 0.2,\n",
    "        \"filter_num\": 400,\n",
    "        \"window_size\": 3,\n",
    "        \"cnn_activation\": \"relu\",\n",
    "        \"model_type\": \"npa\"\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 4,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"loss\": \"cross_entropy_loss\",\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"support_quick_scoring\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "# Guardar en YAML\n",
    "with open(output_path, \"w\") as f:\n",
    "    yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"Fichero YAML generado en: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f97a38a-6d9a-4251-9d22-601944da56ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.9 (reco_env)",
   "language": "python",
   "name": "reco_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
