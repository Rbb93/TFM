{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f02b5faa-b7f8-45f4-b98c-9933870c0713",
   "metadata": {},
   "source": [
    "## Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebc77317-e280-4d7b-b40f-053dc286b6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs detectadas: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "cuDNN versión: 64_8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import recommenders\n",
    "from sklearn.metrics import ndcg_score\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Comprobamos acceso a grafica\n",
    "import tensorflow as tf\n",
    "print(\"GPUs detectadas:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"cuDNN versión:\", tf.sysconfig.get_build_info().get(\"cudnn_version\", \"no detectado\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2565365-77d1-40e1-b51f-d862938377aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 1. RUTAS DEL DATASET\n",
    "# ------------------------------\n",
    "train_path = \"./mind_small/MINDsmall_train\"\n",
    "dev_path = \"./mind_small/MINDsmall_dev\"\n",
    "\n",
    "behaviors_train = os.path.join(train_path, \"behaviors.tsv\")\n",
    "news_train = os.path.join(train_path, \"news.tsv\")\n",
    "\n",
    "behaviors_dev = os.path.join(dev_path, \"behaviors.tsv\")\n",
    "news_dev = os.path.join(dev_path, \"news.tsv\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2. CARGA DE DATOS\n",
    "# ------------------------------\n",
    "train_beh = pd.read_csv(behaviors_train, sep=\"\\t\", header=None,\n",
    "                        names=[\"imp_id\", \"user\", \"time\", \"history\", \"impressions\"])\n",
    "dev_beh = pd.read_csv(behaviors_dev, sep=\"\\t\", header=None,\n",
    "                      names=[\"imp_id\", \"user\", \"time\", \"history\", \"impressions\"])\n",
    "train_news = pd.read_csv(news_train, sep=\"\\t\", header=None,\n",
    "                         names=[\"news_id\", \"category\", \"subcategory\", \"title\", \"abstract\",\n",
    "                                \"url\", \"title_entities\", \"abstract_entities\"])\n",
    "dev_news = pd.read_csv(news_train, sep=\"\\t\", header=None,\n",
    "                         names=[\"news_id\", \"category\", \"subcategory\", \"title\", \"abstract\",\n",
    "                                \"url\", \"title_entities\", \"abstract_entities\"])\n",
    "\n",
    "all_news_ids = train_news[\"news_id\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce434ee9-a211-4371-877e-4b87fb9249f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>title_entities</th>\n",
       "      <th>abstract_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N55528</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>lifestyleroyals</td>\n",
       "      <td>The Brands Queen Elizabeth, Prince Charles, an...</td>\n",
       "      <td>Shop the notebooks, jackets, and more that the...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAGH0ET.html</td>\n",
       "      <td>[{\"Label\": \"Prince Philip, Duke of Edinburgh\",...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N19639</td>\n",
       "      <td>health</td>\n",
       "      <td>weightloss</td>\n",
       "      <td>50 Worst Habits For Belly Fat</td>\n",
       "      <td>These seemingly harmless habits are holding yo...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAB19MK.html</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N61837</td>\n",
       "      <td>news</td>\n",
       "      <td>newsworld</td>\n",
       "      <td>The Cost of Trump's Aid Freeze in the Trenches...</td>\n",
       "      <td>Lt. Ivan Molchanets peeked over a parapet of s...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAJgNsz.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N53526</td>\n",
       "      <td>health</td>\n",
       "      <td>voices</td>\n",
       "      <td>I Was An NBA Wife. Here's How It Affected My M...</td>\n",
       "      <td>I felt like I was a fraud, and being an NBA wi...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AACk2N6.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"National Basketball Association\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N38324</td>\n",
       "      <td>health</td>\n",
       "      <td>medical</td>\n",
       "      <td>How to Get Rid of Skin Tags, According to a De...</td>\n",
       "      <td>They seem harmless, but there's a very good re...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAAKEkt.html</td>\n",
       "      <td>[{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataI...</td>\n",
       "      <td>[{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  news_id   category      subcategory  \\\n",
       "0  N55528  lifestyle  lifestyleroyals   \n",
       "1  N19639     health       weightloss   \n",
       "2  N61837       news        newsworld   \n",
       "3  N53526     health           voices   \n",
       "4  N38324     health          medical   \n",
       "\n",
       "                                               title  \\\n",
       "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
       "1                      50 Worst Habits For Belly Fat   \n",
       "2  The Cost of Trump's Aid Freeze in the Trenches...   \n",
       "3  I Was An NBA Wife. Here's How It Affected My M...   \n",
       "4  How to Get Rid of Skin Tags, According to a De...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Shop the notebooks, jackets, and more that the...   \n",
       "1  These seemingly harmless habits are holding yo...   \n",
       "2  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
       "3  I felt like I was a fraud, and being an NBA wi...   \n",
       "4  They seem harmless, but there's a very good re...   \n",
       "\n",
       "                                             url  \\\n",
       "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
       "1  https://assets.msn.com/labs/mind/AAB19MK.html   \n",
       "2  https://assets.msn.com/labs/mind/AAJgNsz.html   \n",
       "3  https://assets.msn.com/labs/mind/AACk2N6.html   \n",
       "4  https://assets.msn.com/labs/mind/AAAKEkt.html   \n",
       "\n",
       "                                      title_entities  \\\n",
       "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
       "1  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
       "2                                                 []   \n",
       "3                                                 []   \n",
       "4  [{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataI...   \n",
       "\n",
       "                                   abstract_entities  \n",
       "0                                                 []  \n",
       "1  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...  \n",
       "2  [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...  \n",
       "3  [{\"Label\": \"National Basketball Association\", ...  \n",
       "4  [{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataI...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74377e6-7caf-4864-989b-b526e5b85d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------\n",
    "# FUNCIÓN: ordenación aleatoria\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "def sort_impressions_random(candidate_ids):\n",
    "    \"\"\"\n",
    "    Devuelve una permutación aleatoria de los índices de candidate_ids.\n",
    "    Ej.: ['N1','N2','N3'] -> [1,0,2]\n",
    "    \"\"\"\n",
    "    n = len(candidate_ids)\n",
    "    return np.random.permutation(n).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e891aeb1-034c-4f66-8fe0-4239317d7b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@10 promedio Random sobre 100 usuarios: 0.3263093698858336\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# 1. NDCG PROMEDIO SOBRE N USUARIOS\n",
    "# ------------------------------\n",
    "k = 10\n",
    "N = 100  # número de usuarios aleatorios para evaluar\n",
    "ndcgs = []\n",
    "\n",
    "for _ in range(N):\n",
    "    example = dev_beh.sample(n=1).iloc[0]\n",
    "    imps = example[\"impressions\"].split()\n",
    "    candidate_ids = [imp.split(\"-\")[0] for imp in imps]  # todas las noticias mostradas\n",
    "    clicked = [imp.split(\"-\")[0] for imp in imps if imp.endswith(\"-1\")]  # clics reales\n",
    "\n",
    "    if len(clicked) == 0:\n",
    "        continue  # saltar usuarios sin clics\n",
    "\n",
    "    # Predicción Random dentro de los candidates\n",
    "    pred = sort_impressions_random(candidate_ids)[:k]\n",
    "\n",
    "    # Vector de relevancia real (1 si clicada, 0 si no)\n",
    "    y_true = np.array([1 if nid in clicked else 0 for nid in candidate_ids])\n",
    "\n",
    "    # Vector de score para predicción (1 si predicha, 0 si no)\n",
    "    y_score = np.array([1 if nid in pred else 0 for nid in candidate_ids])\n",
    "\n",
    "    ndcgs.append(ndcg_score([y_true], [y_score], k=k))\n",
    "\n",
    "# ------------------------------\n",
    "# 2. RESULTADOS\n",
    "# ------------------------------\n",
    "print(f\"NDCG@{k} promedio Random sobre {len(ndcgs)} usuarios:\", np.mean(ndcgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72a0518e-e8c0-453b-b89a-e2cc34754140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73152it [00:02, 27262.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# GENERAR ARCHIVO random_pred_small.json\n",
    "# ----------------------------------------------------------\n",
    "with open('./mind_small/recommendations/random_pred_small.json', 'w') as f:\n",
    "    for impr_index, (_, row) in tqdm(enumerate(dev_beh.iterrows())):\n",
    "\n",
    "        imps = row[\"impressions\"].split()\n",
    "        candidate_ids = [imp.split(\"-\")[0] for imp in imps]\n",
    "\n",
    "        # ranking aleatorio (lista de índices)\n",
    "        pred_rank = sort_impressions_random(candidate_ids)\n",
    "\n",
    "        # construcción del JSON\n",
    "        obj = {\n",
    "            \"impr_index\": int(impr_index + 1),\n",
    "            \"pred_rank\": pred_rank\n",
    "        }\n",
    "\n",
    "        # escritura: una línea por objeto\n",
    "        f.write(json.dumps(obj) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96cb1d2-8708-41ef-b7c4-40573ad0b074",
   "metadata": {},
   "source": [
    "## Most popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "680f4f40-fe0b-4eb7-ba3a-1f750c8a0bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs detectadas: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "cuDNN versión: 64_8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import recommenders\n",
    "from sklearn.metrics import ndcg_score\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Comprobamos acceso a grafica\n",
    "import tensorflow as tf\n",
    "print(\"GPUs detectadas:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"cuDNN versión:\", tf.sysconfig.get_build_info().get(\"cudnn_version\", \"no detectado\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fada42b5-fae1-4e4d-a012-48c285d026ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 1. RUTAS DEL DATASET\n",
    "# ------------------------------\n",
    "train_path = \"./mind_small/MINDsmall_train\"\n",
    "dev_path = \"./mind_small/MINDsmall_dev\"\n",
    "\n",
    "behaviors_train = os.path.join(train_path, \"behaviors.tsv\")\n",
    "news_train = os.path.join(train_path, \"news.tsv\")\n",
    "\n",
    "behaviors_dev = os.path.join(dev_path, \"behaviors.tsv\")\n",
    "news_dev = os.path.join(dev_path, \"news.tsv\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2. CARGA DE DATOS\n",
    "# ------------------------------\n",
    "train_beh = pd.read_csv(behaviors_train, sep=\"\\t\", header=None,\n",
    "                        names=[\"imp_id\", \"user\", \"time\", \"history\", \"impressions\"])\n",
    "dev_beh = pd.read_csv(behaviors_dev, sep=\"\\t\", header=None,\n",
    "                      names=[\"imp_id\", \"user\", \"time\", \"history\", \"impressions\"])\n",
    "train_news = pd.read_csv(news_train, sep=\"\\t\", header=None,\n",
    "                         names=[\"news_id\", \"category\", \"subcategory\", \"title\", \"abstract\",\n",
    "                                \"url\", \"title_entities\", \"abstract_entities\"])\n",
    "dev_news = pd.read_csv(news_train, sep=\"\\t\", header=None,\n",
    "                         names=[\"news_id\", \"category\", \"subcategory\", \"title\", \"abstract\",\n",
    "                                \"url\", \"title_entities\", \"abstract_entities\"])\n",
    "\n",
    "all_news_ids = train_news[\"news_id\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "377aded2-35c6-409c-a9af-a1f20d61ee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 1. CALCULAR POPULARIDAD DE LAS NOTICIAS\n",
    "# ------------------------------\n",
    "\n",
    "def ranking_most_popular(behaviors):\n",
    "    # asegurarse de no tener NaNs que rompan el split\n",
    "    imprs = behaviors[\"impressions\"].fillna(\"\")\n",
    "    \n",
    "    all_clicked_ids = []   # lista con repeticiones: un elemento por cada \"-1\"\n",
    "    all_shown_ids = set()  # conjunto con todos los ids que se han mostrado alguna vez\n",
    "    \n",
    "    for cell in imprs:\n",
    "        if not cell:\n",
    "            continue\n",
    "        for token in cell.split():\n",
    "            if not token:\n",
    "                continue\n",
    "            try:\n",
    "                nid, flag = token.rsplit(\"-\", 1)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            all_shown_ids.add(nid)\n",
    "            if flag == \"1\":\n",
    "                all_clicked_ids.append(nid)\n",
    "    \n",
    "    # contar clics\n",
    "    popularity = pd.Series(all_clicked_ids).value_counts()  # Serie: ID → clicks\n",
    "    \n",
    "    clicked_ranking = popularity.index.tolist()      # IDs con clics, ordenados\n",
    "    clicked_counts  = popularity.values.tolist()     # nº de clics en el mismo orden\n",
    "    \n",
    "    # IDs que nunca tuvieron clic\n",
    "    never_clicked = list(all_shown_ids - set(clicked_ranking))\n",
    "    never_clicked_counts = [0] * len(never_clicked)\n",
    "    \n",
    "    # combinar\n",
    "    most_popular_ranking = clicked_ranking + never_clicked\n",
    "    click_counts = clicked_counts + never_clicked_counts\n",
    "    rank_pos = {nid: i for i, nid in enumerate(most_popular_ranking)}\n",
    "    \n",
    "    return most_popular_ranking, click_counts, rank_pos\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 2. FUNCIÓN: ordenar por popularidad\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "def sort_impressions_popularity(candidate_ids, rank_pos):\n",
    "    \"\"\"\n",
    "    Devuelve una lista de índices que ordena candidate_ids\n",
    "    según la popularidad global (clics). Las no clicadas van al final.\n",
    "    \n",
    "    candidate_ids: lista de IDs mostrados en una impresión.\n",
    "    return: lista de índices ordenados.\n",
    "    \"\"\"\n",
    "    \n",
    "    # sort basado en la posición del ranking global\n",
    "    sorted_indices = sorted(\n",
    "        range(len(candidate_ids)),\n",
    "        key=lambda i: rank_pos.get(candidate_ids[i], 10**6)  # num enorme → fondo\n",
    "    )\n",
    "    # convertimos a índices empezando en 1\n",
    "    sorted_one_based = [i+1 for i in sorted_indices]\n",
    "    \n",
    "    return sorted_one_based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4b6414e-b320-4d24-8361-a472b0aad899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank_pos es diccionario → posición en el ranking\n",
    "ranking, click_counts, rank_pos = ranking_most_popular(train_beh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41312822-c7e6-4b44-ad30-57755d5e7a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@10 promedio Most Popular sobre 100 usuarios: 0.2820263548761111\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# 1. NDCG PROMEDIO SOBRE N USUARIOS\n",
    "# ------------------------------\n",
    "k = 10\n",
    "N = 100\n",
    "ndcgs = []\n",
    "\n",
    "for _ in range(N):\n",
    "    example = dev_beh.sample(n=1).iloc[0]\n",
    "    imps = example[\"impressions\"].split()\n",
    "    candidate_ids = [imp.split(\"-\")[0] for imp in imps]\n",
    "    clicked = [imp.split(\"-\")[0] for imp in imps if imp.endswith(\"-1\")]\n",
    "\n",
    "    if len(clicked) == 0:\n",
    "        continue\n",
    "\n",
    "    # Predicción Most Popular dentro de los candidates\n",
    "    pred_pop = sort_impressions_popularity(candidate_ids, rank_pos)[:k]\n",
    "\n",
    "    # Vector de relevancia real\n",
    "    y_true = np.array([1 if nid in clicked else 0 for nid in candidate_ids])\n",
    "\n",
    "    # Vector de score para predicción\n",
    "    y_score_pop = np.array([1 if nid in pred_pop else 0 for nid in candidate_ids])\n",
    "\n",
    "    ndcgs.append(ndcg_score([y_true], [y_score_pop], k=k))\n",
    "\n",
    "# ------------------------------\n",
    "# 2. RESULTADOS\n",
    "# ------------------------------\n",
    "print(f\"NDCG@{k} promedio Most Popular sobre {len(ndcgs)} usuarios:\", np.mean(ndcgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b274cd7-46f7-4457-ae46-31165f4bcb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 73152/73152 [00:03<00:00, 24281.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# GENERAR ARCHIVO most_popular_pred_small.json\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "output_file = \"./mind_small/recommendations/pop_pred_small.json\"\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    for impr_index, (_, row) in tqdm(enumerate(dev_beh.iterrows()), total=len(dev_beh)):\n",
    "        \n",
    "        imps = row[\"impressions\"].split()\n",
    "        candidate_ids = [imp.split(\"-\")[0] for imp in imps]\n",
    "\n",
    "        # ranking por popularidad\n",
    "        pred_rank = sort_impressions_popularity(candidate_ids, rank_pos)\n",
    "\n",
    "        # construcción del JSON\n",
    "        obj = {\n",
    "            \"impr_index\": int(impr_index + 1),\n",
    "            \"pred_rank\": pred_rank\n",
    "        }\n",
    "\n",
    "        f.write(json.dumps(obj) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8327cbe5-3982-4c4d-9baa-6e3bbf51f6f6",
   "metadata": {},
   "source": [
    "## LSTUR: Neural News Recommendation with Long- and Short-term User Representations\n",
    "\n",
    "- LSTUR captures both the user’s long-term preferences and short-term interests.\n",
    "- It uses user ID embeddings to learn long-term representations.\n",
    "- It uses the news recently read by the user, processed through a GRU network, to learn short-term representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "650e04b3-8e72-4385-bae8-9e694d00407a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.9.25 (main, Nov  3 2025, 22:44:01) [MSC v.1929 64 bit (AMD64)]\n",
      "Tensorflow version: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "from tempfile import TemporaryDirectory\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR') # only show error messages\n",
    "\n",
    "from recommenders.models.deeprec.deeprec_utils import download_deeprec_resources \n",
    "from recommenders.models.newsrec.newsrec_utils import prepare_hparams\n",
    "from recommenders.models.newsrec.models.lstur import LSTURModel\n",
    "from recommenders.models.newsrec.io.mind_iterator import MINDIterator\n",
    "from recommenders.models.newsrec.newsrec_utils import get_mind_data_set\n",
    "from recommenders.utils.notebook_utils import store_metadata\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "38de168b-02aa-4aa4-bc9b-9d68e84c3aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 4\n",
    "seed = 42\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7a000aca-25a4-43ad-8f5b-5b22000e6e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carpeta local donde tienes los datasets\n",
    "data_path = './mind_small'\n",
    "\n",
    "# Archivos de entrenamiento\n",
    "train_news_file = os.path.join(data_path, 'MINDsmall_train', 'news.tsv')\n",
    "train_behaviors_file = os.path.join(data_path, 'MINDsmall_train', 'behaviors.tsv')\n",
    "\n",
    "# Archivos de validación\n",
    "valid_news_file = os.path.join(data_path, 'MINDsmall_dev', 'news.tsv')\n",
    "valid_behaviors_file = os.path.join(data_path, 'MINDsmall_dev', 'behaviors.tsv')\n",
    "\n",
    "# Archivos de utilidades (embedding, diccionarios, yaml)\n",
    "utils_path = os.path.join(data_path, 'utils')\n",
    "wordDict_file = os.path.join(utils_path, \"word_dict.pkl\")\n",
    "wordEmb_file = os.path.join(utils_path, \"embedding.npy\")\n",
    "userDict_file = os.path.join(utils_path, \"uid2index.pkl\")\n",
    "yaml_file = os.path.join(utils_path, 'lstur.yaml')\n",
    "\n",
    "# Verificamos que existan los archivos\n",
    "for f in [train_news_file, train_behaviors_file, valid_news_file, valid_behaviors_file,\n",
    "          wordEmb_file, userDict_file, wordDict_file, yaml_file]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"Archivo no encontrado: {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc8e17ad-c4fb-43f2-88f3-bd503d5c9627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HParams object with values {'support_quick_scoring': True, 'dropout': 0.2, 'attention_hidden_dim': 200, 'head_num': 4, 'head_dim': 100, 'filter_num': 400, 'window_size': 3, 'vert_emb_dim': 100, 'subvert_emb_dim': 100, 'gru_unit': 400, 'type': 'ini', 'user_emb_dim': 50, 'learning_rate': 0.0001, 'optimizer': 'adam', 'epochs': 4, 'batch_size': 32, 'show_step': 100000, 'title_size': 30, 'his_size': 50, 'data_format': 'news', 'npratio': 4, 'metrics': ['group_auc', 'mean_mrr', 'ndcg@5;10'], 'word_emb_dim': 300, 'cnn_activation': 'relu', 'model_type': 'lstur', 'loss': 'cross_entropy_loss', 'wordEmb_file': './mind_small\\\\utils\\\\embedding.npy', 'wordDict_file': './mind_small\\\\utils\\\\word_dict.pkl', 'userDict_file': './mind_small\\\\utils\\\\uid2index.pkl'}\n"
     ]
    }
   ],
   "source": [
    "hparams = prepare_hparams(yaml_file, \n",
    "                          wordEmb_file=wordEmb_file,\n",
    "                          wordDict_file=wordDict_file, \n",
    "                          userDict_file=userDict_file,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=epochs)\n",
    "print(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad86015d-3282-40cb-a4cf-aa6d1450ff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = MINDIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fc46473-30c8-4cf8-8ad9-9f4ac0e59146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv1d/Relu:0\", shape=(None, 30, 400), dtype=float32)\n",
      "Tensor(\"att_layer2/Sum_1:0\", shape=(None, 400), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\reco_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = LSTURModel(hparams, iterator, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "510582cf-8e92-4dbe-b755-ea37ed67d34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\User\\anaconda3\\envs\\reco_env\\lib\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "1326it [00:06, 214.17it/s] \n",
      "2286it [01:18, 29.22it/s]\n",
      "73152it [00:04, 14745.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'group_auc': 0.4802, 'mean_mrr': 0.206, 'ndcg@5': 0.2113, 'ndcg@10': 0.2711}\n",
      "CPU times: total: 1min 46s\n",
      "Wall time: 2min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Valores previos a entrenarse\n",
    "print(model.run_eval(valid_news_file, valid_behaviors_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aca37604-efde-4f27-aede-041c3ffa5529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7386it [24:44,  4.97it/s]\n",
      "1326it [00:01, 1126.87it/s]\n",
      "2286it [01:13, 30.99it/s]\n",
      "73152it [00:04, 15421.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 1\n",
      "train info: logloss loss:1.3824320930292573\n",
      "eval info: group_auc:0.6498, mean_mrr:0.3049, ndcg@10:0.398, ndcg@5:0.334\n",
      "at epoch 1 , train time: 1484.8 eval time: 123.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7386it [24:31,  5.02it/s]\n",
      "1326it [00:01, 1150.74it/s]\n",
      "2286it [01:13, 31.09it/s]\n",
      "73152it [00:04, 15230.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 2\n",
      "train info: logloss loss:1.29995469326512\n",
      "eval info: group_auc:0.661, mean_mrr:0.3138, ndcg@10:0.4074, ndcg@5:0.344\n",
      "at epoch 2 , train time: 1471.1 eval time: 124.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7386it [24:30,  5.02it/s]\n",
      "1326it [00:01, 1136.28it/s]\n",
      "2286it [01:13, 30.98it/s]\n",
      "73152it [00:05, 14462.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 3\n",
      "train info: logloss loss:1.2024294150463133\n",
      "eval info: group_auc:0.6569, mean_mrr:0.3115, ndcg@10:0.4043, ndcg@5:0.3418\n",
      "at epoch 3 , train time: 1470.7 eval time: 124.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7386it [24:31,  5.02it/s]\n",
      "1326it [00:01, 1172.85it/s]\n",
      "2286it [01:13, 31.31it/s]\n",
      "73152it [00:04, 15168.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 4\n",
      "train info: logloss loss:1.1086378012719014\n",
      "eval info: group_auc:0.646, mean_mrr:0.303, ndcg@10:0.3943, ndcg@5:0.3299\n",
      "at epoch 4 , train time: 1471.6 eval time: 123.4\n",
      "CPU times: total: 2h 15min 9s\n",
      "Wall time: 1h 46min 33s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<recommenders.models.newsrec.models.lstur.LSTURModel at 0x1fd8f46e6d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Entreno\n",
    "model.fit(train_news_file, train_behaviors_file, valid_news_file, valid_behaviors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2988f29f-5e4e-439e-9484-ba4ab9d7546b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1326it [00:01, 812.50it/s] \n",
      "2286it [01:13, 31.15it/s]\n",
      "73152it [00:04, 14907.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'group_auc': 0.646, 'mean_mrr': 0.303, 'ndcg@5': 0.3299, 'ndcg@10': 0.3943}\n",
      "CPU times: total: 1min 37s\n",
      "Wall time: 2min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Evaluamos post entreno\n",
    "res_syn = model.run_eval(valid_news_file, valid_behaviors_file)\n",
    "print(res_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4957d80e-ea49-4f77-9380-ddac4c6443e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el modelo\n",
    "model_path = os.path.join(data_path, \"models/lstur\")\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "model.model.save_weights(os.path.join(model_path, \"lstur\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7320804a-e52c-4885-861e-719e157eab87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv1d/Relu:0\", shape=(None, 30, 400), dtype=float32)\n",
      "Tensor(\"att_layer2/Sum_1:0\", shape=(None, 400), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\reco_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos cargados correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Leemos el modelo guardado\n",
    "epochs = 4\n",
    "seed = 42\n",
    "batch_size = 32\n",
    "\n",
    "yaml_file = os.path.join(utils_path, 'lstur.yaml')\n",
    "\n",
    "# 1. Cargar hparams desde YAML\n",
    "hparams = prepare_hparams(yaml_file, \n",
    "                          wordEmb_file=wordEmb_file,\n",
    "                          wordDict_file=wordDict_file, \n",
    "                          userDict_file=userDict_file,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=epochs)\n",
    "\n",
    "iterator = MINDIterator\n",
    "\n",
    "# 2. Crear el modelo vacío\n",
    "model = LSTURModel(hparams, iterator, seed=seed)\n",
    "\n",
    "# 3. Cargar los pesos\n",
    "model.model.load_weights(\"mind_small/models/lstur/lstur\")\n",
    "\n",
    "print(\"Pesos cargados correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d1566636-3f61-4000-bef6-400cbd15e653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1326it [00:02, 475.13it/s]\n",
      "2286it [02:54, 13.10it/s]\n",
      "73152it [00:08, 9021.01it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'group_auc': 0.646, 'mean_mrr': 0.303, 'ndcg@5': 0.3299, 'ndcg@10': 0.3943}\n",
      "CPU times: total: 8min 18s\n",
      "Wall time: 3min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Evaluamos post importacion\n",
    "res_syn = model.run_eval(valid_news_file, valid_behaviors_file)\n",
    "print(res_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "55c0562f-8835-4b82-b8a2-836d00e70844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\User\\anaconda3\\envs\\reco_env\\lib\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "1326it [00:06, 196.98it/s]\n",
      "2286it [02:53, 13.19it/s]\n",
      "73152it [00:05, 14269.97it/s]\n"
     ]
    }
   ],
   "source": [
    "group_impr_indexes, group_labels, group_preds = model.run_fast_eval(valid_news_file, valid_behaviors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2e7c7e6e-88e0-4836-8b5a-42c3e75dd815",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73152it [00:00, 88152.72it/s]\n"
     ]
    }
   ],
   "source": [
    "output_file = os.path.join(data_path, 'recommendations/lstur_pred_small.json')\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    for impr_index, preds in tqdm(zip(group_impr_indexes, group_preds)):\n",
    "        impr_index += 1\n",
    "\n",
    "        # Calcular el ranking\n",
    "        pred_rank = (np.argsort(np.argsort(preds)[::-1]) + 1).tolist()\n",
    "\n",
    "        # Crear estructura JSON\n",
    "        obj = {\n",
    "            \"impr_index\": int(impr_index),\n",
    "            \"pred_rank\": pred_rank\n",
    "        }\n",
    "\n",
    "        # Escribir como JSON en una línea\n",
    "        f.write(json.dumps(obj) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96270eb1-611a-48fd-b83b-e95cf740e0b5",
   "metadata": {},
   "source": [
    "## NRMS: Neural News Recommendation with Multi-Head Self-Attention\n",
    "\n",
    "- NRMS is a **content-based** news recommendation approach.  \n",
    "- It uses **multi-head self-attention** to learn news representations by modeling interactions between words, and to learn user representations by capturing relationships among the news articles they have read.  \n",
    "- It employs **additive attention** to select the most important words and news articles, generating more informative representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "486224bc-f7bc-4c0d-8afe-335e56637974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.9.25 (main, Nov  3 2025, 22:44:01) [MSC v.1929 64 bit (AMD64)]\n",
      "Tensorflow version: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "from tempfile import TemporaryDirectory\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR') # only show error messages\n",
    "\n",
    "from recommenders.models.deeprec.deeprec_utils import download_deeprec_resources \n",
    "from recommenders.models.newsrec.newsrec_utils import prepare_hparams\n",
    "from recommenders.models.newsrec.models.nrms import NRMSModel\n",
    "from recommenders.models.newsrec.io.mind_iterator import MINDIterator\n",
    "from recommenders.models.newsrec.newsrec_utils import get_mind_data_set\n",
    "from recommenders.utils.notebook_utils import store_metadata\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "be3767bd-9913-4d1f-bf74-aaf0c147b8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 4\n",
    "seed = 42\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "852fee0c-cd7b-42a0-bd1c-06faa97a63eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carpeta local donde tienes los datasets\n",
    "data_path = './mind_small'\n",
    "\n",
    "# Archivos de entrenamiento\n",
    "train_news_file = os.path.join(data_path, 'MINDsmall_train', 'news.tsv')\n",
    "train_behaviors_file = os.path.join(data_path, 'MINDsmall_train', 'behaviors.tsv')\n",
    "\n",
    "# Archivos de validación\n",
    "valid_news_file = os.path.join(data_path, 'MINDsmall_dev', 'news.tsv')\n",
    "valid_behaviors_file = os.path.join(data_path, 'MINDsmall_dev', 'behaviors.tsv')\n",
    "\n",
    "# Archivos de utilidades (embedding, diccionarios, yaml)\n",
    "utils_path = os.path.join(data_path, 'utils')\n",
    "wordDict_file = os.path.join(utils_path, \"word_dict.pkl\")\n",
    "wordEmb_file = os.path.join(utils_path, \"embedding.npy\")\n",
    "userDict_file = os.path.join(utils_path, \"uid2index.pkl\")\n",
    "yaml_file = os.path.join(utils_path, 'nrms.yaml')\n",
    "\n",
    "# Verificamos que existan los archivos\n",
    "for f in [train_news_file, train_behaviors_file, valid_news_file, valid_behaviors_file,\n",
    "          wordEmb_file, userDict_file, wordDict_file, yaml_file]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"Archivo no encontrado: {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4822e0ac-7948-4374-a7d1-d70e328af843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HParams object with values {'support_quick_scoring': True, 'dropout': 0.2, 'attention_hidden_dim': 200, 'head_num': 20, 'head_dim': 20, 'filter_num': 200, 'window_size': 3, 'vert_emb_dim': 100, 'subvert_emb_dim': 100, 'gru_unit': 400, 'type': 'ini', 'user_emb_dim': 50, 'learning_rate': 0.0001, 'optimizer': 'adam', 'epochs': 4, 'batch_size': 32, 'show_step': 10, 'title_size': 30, 'his_size': 50, 'data_format': 'news', 'npratio': 4, 'metrics': ['group_auc', 'mean_mrr', 'ndcg@5;10'], 'word_emb_dim': 300, 'model_type': 'nrms', 'loss': 'cross_entropy_loss', 'wordEmb_file': './mind_small\\\\utils\\\\embedding.npy', 'wordDict_file': './mind_small\\\\utils\\\\word_dict.pkl', 'userDict_file': './mind_small\\\\utils\\\\uid2index.pkl'}\n"
     ]
    }
   ],
   "source": [
    "hparams = prepare_hparams(yaml_file, \n",
    "                          wordEmb_file=wordEmb_file,\n",
    "                          wordDict_file=wordDict_file, \n",
    "                          userDict_file=userDict_file,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=epochs,\n",
    "                          show_step=10)\n",
    "print(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "832cbc54-659d-43fa-89c5-9dfdca20b199",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = MINDIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5594c138-de07-472e-a4a0-1e92e3928324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\reco_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = NRMSModel(hparams, iterator, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9da00dd0-ee4e-4ecb-a17f-749492a0cbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\User\\anaconda3\\envs\\reco_env\\lib\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "1326it [00:02, 538.19it/s] \n",
      "2286it [00:52, 43.82it/s]\n",
      "73152it [00:04, 15207.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'group_auc': 0.4976, 'mean_mrr': 0.2183, 'ndcg@5': 0.2219, 'ndcg@10': 0.2855}\n",
      "CPU times: total: 2min 28s\n",
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Valores previos al entreno\n",
    "print(model.run_eval(valid_news_file, valid_behaviors_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "416a5c36-4ab4-44a4-9371-74a6a926acb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step 7380 , total_loss: 1.3963, data_loss: 1.3808: : 7386it [20:16,  6.07it/s]\n",
      "1326it [00:01, 1116.15it/s]\n",
      "2286it [00:50, 45.38it/s]\n",
      "73152it [00:04, 15125.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 1\n",
      "train info: logloss loss:1.3963061434174178\n",
      "eval info: group_auc:0.6366, mean_mrr:0.2949, ndcg@10:0.3874, ndcg@5:0.3216\n",
      "at epoch 1 , train time: 1216.6 eval time: 100.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step 7380 , total_loss: 1.3061, data_loss: 1.3983: : 7386it [20:10,  6.10it/s]\n",
      "1326it [00:01, 1113.70it/s]\n",
      "2286it [00:50, 45.42it/s]\n",
      "73152it [00:04, 14978.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 2\n",
      "train info: logloss loss:1.3061103470242188\n",
      "eval info: group_auc:0.6464, mean_mrr:0.3028, ndcg@10:0.3967, ndcg@5:0.3313\n",
      "at epoch 2 , train time: 1210.7 eval time: 100.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step 7380 , total_loss: 1.2720, data_loss: 1.2002: : 7386it [20:10,  6.10it/s]\n",
      "1326it [00:01, 1110.42it/s]\n",
      "2286it [00:50, 45.41it/s]\n",
      "73152it [00:04, 15375.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 3\n",
      "train info: logloss loss:1.2719256742324023\n",
      "eval info: group_auc:0.6526, mean_mrr:0.3078, ndcg@10:0.4025, ndcg@5:0.336\n",
      "at epoch 3 , train time: 1210.8 eval time: 100.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step 7380 , total_loss: 1.2488, data_loss: 1.3256: : 7386it [20:11,  6.10it/s]\n",
      "1326it [00:01, 1085.80it/s]\n",
      "2286it [00:50, 45.39it/s]\n",
      "73152it [00:05, 14552.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 4\n",
      "train info: logloss loss:1.2487543693022647\n",
      "eval info: group_auc:0.6482, mean_mrr:0.3072, ndcg@10:0.4006, ndcg@5:0.3355\n",
      "at epoch 4 , train time: 1211.4 eval time: 100.9\n",
      "CPU times: total: 1h 33min 30s\n",
      "Wall time: 1h 27min 32s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<recommenders.models.newsrec.models.nrms.NRMSModel at 0x1ff87cd9fd0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Entreno\n",
    "model.fit(train_news_file, train_behaviors_file, valid_news_file, valid_behaviors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89ed16a1-44e5-4db1-934f-e0ce58ff4794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1326it [00:01, 866.26it/s] \n",
      "2286it [00:50, 45.43it/s]\n",
      "73152it [00:04, 15039.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'group_auc': 0.6482, 'mean_mrr': 0.3072, 'ndcg@5': 0.3355, 'ndcg@10': 0.4006}\n",
      "CPU times: total: 2min 26s\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Evaluamos post entreno\n",
    "res_syn = model.run_eval(valid_news_file, valid_behaviors_file)\n",
    "print(res_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b04e7790-a774-46f2-9365-0f424e3e8241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el modelo\n",
    "model_path = os.path.join(data_path, \"models/nrms\")\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "model.model.save_weights(os.path.join(model_path, \"nrms\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "819e23d2-1067-4668-83ba-5ec36091c585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos cargados correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Leemos el modelo guardado\n",
    "epochs = 4\n",
    "seed = 42\n",
    "batch_size = 32\n",
    "\n",
    "yaml_file = os.path.join(utils_path, 'nrms.yaml')\n",
    "\n",
    "# 1. Cargar hparams desde YAML\n",
    "hparams = prepare_hparams(yaml_file, \n",
    "                          wordEmb_file=wordEmb_file,\n",
    "                          wordDict_file=wordDict_file, \n",
    "                          userDict_file=userDict_file,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=epochs,\n",
    "                          show_step=10)\n",
    "\n",
    "iterator = MINDIterator\n",
    "\n",
    "# 2. Crear el modelo vacío\n",
    "model = NRMSModel(hparams, iterator, seed=seed)\n",
    "\n",
    "# 3. Cargar los pesos\n",
    "model.model.load_weights(\"mind_small/models/nrms/nrms\")\n",
    "\n",
    "print(\"Pesos cargados correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7b1d5be4-2325-4db8-b36e-c0852afd1bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1326it [00:02, 530.81it/s]\n",
      "2286it [00:55, 41.50it/s]\n",
      "73152it [00:05, 14306.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'group_auc': 0.6482, 'mean_mrr': 0.3072, 'ndcg@5': 0.3355, 'ndcg@10': 0.4006}\n",
      "CPU times: total: 2min 37s\n",
      "Wall time: 1min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Evaluamos post importacion\n",
    "res_syn = model.run_eval(valid_news_file, valid_behaviors_file)\n",
    "print(res_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "402da515-ad22-459a-88b7-f96f96298f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1326it [00:01, 736.23it/s]\n",
      "2286it [00:52, 43.14it/s]\n",
      "73152it [00:05, 14105.24it/s]\n"
     ]
    }
   ],
   "source": [
    "group_impr_indexes, group_labels, group_preds = model.run_fast_eval(valid_news_file, valid_behaviors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d49ea4c0-79c5-425e-9a0a-0b052d56a53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73152it [00:00, 80838.10it/s]\n"
     ]
    }
   ],
   "source": [
    "output_file = os.path.join(data_path, 'recommendations/nrms_pred_small.json')\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    for impr_index, preds in tqdm(zip(group_impr_indexes, group_preds)):\n",
    "        impr_index += 1\n",
    "\n",
    "        # Calcular el ranking\n",
    "        pred_rank = (np.argsort(np.argsort(preds)[::-1]) + 1).tolist()\n",
    "\n",
    "        # Crear estructura JSON\n",
    "        obj = {\n",
    "            \"impr_index\": int(impr_index),\n",
    "            \"pred_rank\": pred_rank\n",
    "        }\n",
    "\n",
    "        # Escribir como JSON en una línea\n",
    "        f.write(json.dumps(obj) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdc9db4-a88d-4a69-9ffb-b9434181652e",
   "metadata": {},
   "source": [
    "## NAML: Neural News Recommendation with Attentive Multi-View Learning\n",
    "\n",
    "- **NAML is a neural, multi-view news recommendation approach.**\n",
    "- **It uses the news title, body, category, and subcategory** to obtain the news representation.  \n",
    "  It also uses the user’s behavior history to learn the user representation.\n",
    "- **NAML employs additive attention** to learn informative representations by selecting the most relevant words and news articles.\n",
    "- **Due to legal issues**, the MIND dataset does not release the full news bodies.  \n",
    "  Therefore, **abstracts** are used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0600256-0eb9-4464-aeb2-dfb2fa51f26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.9.25 (main, Nov  3 2025, 22:44:01) [MSC v.1929 64 bit (AMD64)]\n",
      "Tensorflow version: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "from tempfile import TemporaryDirectory\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR') # only show error messages\n",
    "\n",
    "from recommenders.models.deeprec.deeprec_utils import download_deeprec_resources \n",
    "from recommenders.models.newsrec.newsrec_utils import prepare_hparams\n",
    "from recommenders.models.newsrec.models.naml import NAMLModel\n",
    "from recommenders.models.newsrec.io.mind_all_iterator import MINDAllIterator\n",
    "from recommenders.models.newsrec.newsrec_utils import get_mind_data_set\n",
    "from recommenders.utils.notebook_utils import store_metadata\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bd5ecd1-2c18-4419-b4f3-ddc9357f9167",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 4\n",
    "seed = 42\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dbe869a-00c1-4b96-8819-07fd8691989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carpeta local donde tienes los datasets\n",
    "data_path = './mind_small'\n",
    "\n",
    "# Archivos de entrenamiento\n",
    "train_news_file = os.path.join(data_path, 'MINDsmall_train', 'news.tsv')\n",
    "train_behaviors_file = os.path.join(data_path, 'MINDsmall_train', 'behaviors.tsv')\n",
    "\n",
    "# Archivos de validación\n",
    "valid_news_file = os.path.join(data_path, 'MINDsmall_dev', 'news.tsv')\n",
    "valid_behaviors_file = os.path.join(data_path, 'MINDsmall_dev', 'behaviors.tsv')\n",
    "\n",
    "# Archivos de utilidades (embedding, diccionarios, yaml)\n",
    "utils_path = os.path.join(data_path, 'utils')\n",
    "wordDict_file = os.path.join(utils_path, \"word_dict_all.pkl\")\n",
    "wordEmb_file = os.path.join(utils_path, \"embedding_all.npy\")\n",
    "userDict_file = os.path.join(utils_path, \"uid2index.pkl\")\n",
    "vertDict_file = os.path.join(utils_path, \"vert_dict.pkl\")\n",
    "subvertDict_file = os.path.join(utils_path, \"subvert_dict.pkl\")\n",
    "yaml_file = os.path.join(utils_path, 'naml.yaml')\n",
    "\n",
    "# Verificamos que existan los archivos\n",
    "for f in [train_news_file, train_behaviors_file, valid_news_file, valid_behaviors_file,\n",
    "          wordEmb_file, userDict_file, wordDict_file, yaml_file]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"Archivo no encontrado: {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52362953-94b7-49f6-a00c-64bf08822509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HParams object with values {'support_quick_scoring': True, 'dropout': 0.2, 'attention_hidden_dim': 200, 'head_num': 4, 'head_dim': 100, 'filter_num': 400, 'window_size': 3, 'vert_emb_dim': 100, 'subvert_emb_dim': 100, 'gru_unit': 400, 'type': 'ini', 'user_emb_dim': 50, 'learning_rate': 0.0001, 'optimizer': 'adam', 'epochs': 4, 'batch_size': 32, 'show_step': 100000, 'title_size': 30, 'body_size': 50, 'his_size': 50, 'vert_num': 17, 'subvert_num': 264, 'data_format': 'naml', 'npratio': 4, 'metrics': ['group_auc', 'mean_mrr', 'ndcg@5;10'], 'word_emb_dim': 300, 'cnn_activation': 'relu', 'model_type': 'naml', 'dense_activation': 'relu', 'loss': 'cross_entropy_loss', 'wordEmb_file': './mind_small\\\\utils\\\\embedding_all.npy', 'wordDict_file': './mind_small\\\\utils\\\\word_dict_all.pkl', 'userDict_file': './mind_small\\\\utils\\\\uid2index.pkl', 'vertDict_file': './mind_small\\\\utils\\\\vert_dict.pkl', 'subvertDict_file': './mind_small\\\\utils\\\\subvert_dict.pkl'}\n"
     ]
    }
   ],
   "source": [
    "hparams = prepare_hparams(yaml_file, \n",
    "                          wordEmb_file=wordEmb_file,\n",
    "                          wordDict_file=wordDict_file, \n",
    "                          userDict_file=userDict_file,\n",
    "                          vertDict_file=vertDict_file, \n",
    "                          subvertDict_file=subvertDict_file,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=epochs)\n",
    "print(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b18f0a46-4df4-4461-8883-a43a3d110e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = MINDAllIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71a5c7ee-1893-4cf3-9d3f-0c608ea79a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NAMLModel(hparams, iterator, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d427d17a-d080-4347-bccd-7d7cc6ea6d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\User\\anaconda3\\envs\\reco_env\\lib\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "42386it [01:24, 498.72it/s]\n",
      "73121it [02:51, 427.27it/s]\n",
      "73152it [00:04, 14785.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'group_auc': 0.4888, 'mean_mrr': 0.2074, 'ndcg@5': 0.2098, 'ndcg@10': 0.2753}\n",
      "CPU times: total: 8min 43s\n",
      "Wall time: 5min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Valores previos al entreno\n",
    "print(model.run_eval(valid_news_file, valid_behaviors_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7e3698ec-b48e-4dee-8ea2-1856b12209f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7385it [39:41,  3.10it/s]\n",
      "42386it [01:23, 508.07it/s]\n",
      "73121it [02:50, 428.95it/s]\n",
      "73152it [00:05, 14358.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 1\n",
      "train info: logloss loss:1.3725701858889192\n",
      "eval info: group_auc:0.6467, mean_mrr:0.3023, ndcg@10:0.3953, ndcg@5:0.3341\n",
      "at epoch 1 , train time: 2381.6 eval time: 303.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7385it [39:32,  3.11it/s]\n",
      "42386it [01:23, 506.17it/s]\n",
      "73121it [02:49, 432.42it/s]\n",
      "73152it [00:05, 14135.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 2\n",
      "train info: logloss loss:1.3085053350446025\n",
      "eval info: group_auc:0.6546, mean_mrr:0.3062, ndcg@10:0.4019, ndcg@5:0.3413\n",
      "at epoch 2 , train time: 2372.4 eval time: 302.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7385it [39:33,  3.11it/s]\n",
      "42386it [01:23, 510.22it/s]\n",
      "73121it [02:48, 433.05it/s]\n",
      "73152it [00:05, 14312.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 3\n",
      "train info: logloss loss:1.2834909335212603\n",
      "eval info: group_auc:0.6599, mean_mrr:0.3129, ndcg@10:0.4081, ndcg@5:0.3476\n",
      "at epoch 3 , train time: 2373.9 eval time: 301.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7385it [39:32,  3.11it/s]\n",
      "42386it [01:23, 507.42it/s]\n",
      "73121it [02:49, 430.55it/s]\n",
      "73152it [00:05, 14280.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 4\n",
      "train info: logloss loss:1.265041159187206\n",
      "eval info: group_auc:0.661, mean_mrr:0.313, ndcg@10:0.408, ndcg@5:0.3455\n",
      "at epoch 4 , train time: 2372.9 eval time: 302.8\n",
      "CPU times: total: 3h 19min 34s\n",
      "Wall time: 2h 58min 30s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<recommenders.models.newsrec.models.naml.NAMLModel at 0x200a80ce670>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Entreno\n",
    "model.fit(train_news_file, train_behaviors_file, valid_news_file, valid_behaviors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "451a9df1-ce59-4244-b3c7-08adb7a27830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42386it [01:23, 506.33it/s]\n",
      "73121it [02:49, 430.23it/s]\n",
      "73152it [00:05, 14575.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'group_auc': 0.661, 'mean_mrr': 0.313, 'ndcg@5': 0.3455, 'ndcg@10': 0.408}\n",
      "CPU times: total: 8min 38s\n",
      "Wall time: 5min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Evaluamos post entreno\n",
    "res_syn = model.run_eval(valid_news_file, valid_behaviors_file)\n",
    "print(res_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "51028f7a-0e22-45db-a255-748c7612773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el modelo\n",
    "model_path = os.path.join(data_path, \"models/naml\")\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "model.model.save_weights(os.path.join(model_path, \"naml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "abef23f7-01f8-4828-8693-6db8ebcd8722",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\reco_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos cargados correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Leemos el modelo guardado\n",
    "epochs = 4\n",
    "seed = 42\n",
    "batch_size = 32\n",
    "\n",
    "yaml_file = os.path.join(utils_path, 'naml.yaml')\n",
    "\n",
    "# 1. Cargar hparams desde YAML\n",
    "hparams = prepare_hparams(yaml_file, \n",
    "                          wordEmb_file=wordEmb_file,\n",
    "                          wordDict_file=wordDict_file, \n",
    "                          userDict_file=userDict_file,\n",
    "                          vertDict_file=vertDict_file, \n",
    "                          subvertDict_file=subvertDict_file,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=epochs)\n",
    "\n",
    "iterator = MINDAllIterator\n",
    "\n",
    "# 2. Crear el modelo vacío\n",
    "model = NAMLModel(hparams, iterator, seed=seed)\n",
    "\n",
    "# 3. Cargar los pesos\n",
    "model.model.load_weights(\"mind_small/models/naml/naml\")\n",
    "\n",
    "print(\"Pesos cargados correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f0fd8cd9-8099-4b0b-968a-90dfafbed237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\User\\anaconda3\\envs\\reco_env\\lib\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "42386it [02:00, 352.74it/s]\n",
      "73121it [03:57, 307.37it/s]\n",
      "73152it [00:05, 13330.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'group_auc': 0.661, 'mean_mrr': 0.313, 'ndcg@5': 0.3455, 'ndcg@10': 0.408}\n",
      "CPU times: total: 12min 50s\n",
      "Wall time: 6min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Evaluamos post importacion\n",
    "res_syn = model.run_eval(valid_news_file, valid_behaviors_file)\n",
    "print(res_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fd7b5d8b-5754-4f95-99b9-419be37fbf55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\User\\anaconda3\\envs\\reco_env\\lib\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "42386it [01:59, 355.10it/s]\n",
      "73121it [03:44, 325.82it/s]\n",
      "73152it [00:04, 14638.09it/s]\n"
     ]
    }
   ],
   "source": [
    "group_impr_indexes, group_labels, group_preds = model.run_fast_eval(valid_news_file, valid_behaviors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b5a2c79b-6a8c-4aab-8c32-013e165e4bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73152it [00:00, 89884.71it/s]\n"
     ]
    }
   ],
   "source": [
    "output_file = os.path.join(data_path, 'recommendations/naml_pred_small.json')\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    for impr_index, preds in tqdm(zip(group_impr_indexes, group_preds)):\n",
    "        impr_index += 1\n",
    "\n",
    "        # Calcular el ranking\n",
    "        pred_rank = (np.argsort(np.argsort(preds)[::-1]) + 1).tolist()\n",
    "\n",
    "        # Crear estructura JSON\n",
    "        obj = {\n",
    "            \"impr_index\": int(impr_index),\n",
    "            \"pred_rank\": pred_rank\n",
    "        }\n",
    "\n",
    "        # Escribir como JSON en una línea\n",
    "        f.write(json.dumps(obj) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7a2335-f8b5-422b-9801-2bcc3aea63ba",
   "metadata": {},
   "source": [
    "## NPA: Neural News Recommendation with Personalized Attention\n",
    "\n",
    "- **NPA is a content-based news recommendation method.**\n",
    "- **It uses a CNN** to learn news representations and learns user representations from the news articles they have clicked.\n",
    "- **Personalized attention is applied at the word level** so that the model highlights important words according to the user.\n",
    "- **Personalized attention is applied at the news level** so that the model highlights the most relevant historical news articles according to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b681f74f-2426-4f6b-a3db-922369dc3178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.9.25 (main, Nov  3 2025, 22:44:01) [MSC v.1929 64 bit (AMD64)]\n",
      "Tensorflow version: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "from tempfile import TemporaryDirectory\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR') # only show error messages\n",
    "\n",
    "from recommenders.models.deeprec.deeprec_utils import download_deeprec_resources \n",
    "from recommenders.models.newsrec.newsrec_utils import prepare_hparams\n",
    "from recommenders.models.newsrec.models.npa import NPAModel\n",
    "from recommenders.models.newsrec.io.mind_iterator import MINDIterator\n",
    "from recommenders.models.newsrec.newsrec_utils import get_mind_data_set\n",
    "from recommenders.utils.notebook_utils import store_metadata\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b12387a9-9a0d-4e52-8f16-6eee2cda8b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 4\n",
    "seed = 42\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ae5a349-9552-4b23-819f-b081b868544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carpeta local donde tienes los datasets\n",
    "data_path = './mind_small'\n",
    "\n",
    "# Archivos de entrenamiento\n",
    "train_news_file = os.path.join(data_path, 'MINDsmall_train', 'news.tsv')\n",
    "train_behaviors_file = os.path.join(data_path, 'MINDsmall_train', 'behaviors.tsv')\n",
    "\n",
    "# Archivos de validación\n",
    "valid_news_file = os.path.join(data_path, 'MINDsmall_dev', 'news.tsv')\n",
    "valid_behaviors_file = os.path.join(data_path, 'MINDsmall_dev', 'behaviors.tsv')\n",
    "\n",
    "# Archivos de utilidades (embedding, diccionarios, yaml)\n",
    "utils_path = os.path.join(data_path, 'utils')\n",
    "wordDict_file = os.path.join(utils_path, \"word_dict.pkl\")\n",
    "wordEmb_file = os.path.join(utils_path, \"embedding.npy\")\n",
    "userDict_file = os.path.join(utils_path, \"uid2index.pkl\")\n",
    "yaml_file = os.path.join(utils_path, 'npa.yaml')\n",
    "\n",
    "# Verificamos que existan los archivos\n",
    "for f in [train_news_file, train_behaviors_file, valid_news_file, valid_behaviors_file,\n",
    "          wordEmb_file, userDict_file, wordDict_file, yaml_file]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"Archivo no encontrado: {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "268cc1c8-f717-4737-a3aa-ccaa8e391c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HParams object with values {'support_quick_scoring': False, 'dropout': 0.2, 'attention_hidden_dim': 200, 'head_num': 4, 'head_dim': 100, 'filter_num': 400, 'window_size': 3, 'vert_emb_dim': 100, 'subvert_emb_dim': 100, 'gru_unit': 400, 'type': 'ini', 'user_emb_dim': 100, 'learning_rate': 0.0001, 'optimizer': 'adam', 'epochs': 4, 'batch_size': 32, 'show_step': 100000, 'title_size': 10, 'his_size': 50, 'data_format': 'news', 'npratio': 4, 'metrics': ['group_auc', 'mean_mrr', 'ndcg@5;10'], 'word_emb_dim': 300, 'cnn_activation': 'relu', 'model_type': 'npa', 'loss': 'cross_entropy_loss', 'wordEmb_file': './mind_small\\\\utils\\\\embedding.npy', 'wordDict_file': './mind_small\\\\utils\\\\word_dict.pkl', 'userDict_file': './mind_small\\\\utils\\\\uid2index.pkl'}\n"
     ]
    }
   ],
   "source": [
    "hparams = prepare_hparams(yaml_file, \n",
    "                          wordEmb_file=wordEmb_file,\n",
    "                          wordDict_file=wordDict_file, \n",
    "                          userDict_file=userDict_file,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=epochs)\n",
    "print(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72ce7780-b9f4-44a9-9df2-77818a5901ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = MINDIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2ec53ec-44e7-437b-b66b-e8aac3aed963",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\reco_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = NPAModel(hparams, iterator, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ea259112-271c-42ab-92f2-472ad220a8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\User\\anaconda3\\envs\\reco_env\\lib\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "85657it [06:09, 231.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'group_auc': 0.5048, 'mean_mrr': 0.2248, 'ndcg@5': 0.233, 'ndcg@10': 0.2956}\n",
      "CPU times: total: 9min 36s\n",
      "Wall time: 6min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Evaluamos pre entreno\n",
    "print(model.run_eval(valid_news_file, valid_behaviors_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "48291388-b2fe-4325-8701-f2d2605c0a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7386it [09:36, 12.81it/s]\n",
      "85657it [06:05, 234.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 1\n",
      "train info: logloss loss:1.4270442090414095\n",
      "eval info: group_auc:0.6, mean_mrr:0.2679, ndcg@10:0.3543, ndcg@5:0.2925\n",
      "at epoch 1 , train time: 576.4 eval time: 413.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7386it [09:29, 12.97it/s]\n",
      "85657it [06:05, 234.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 2\n",
      "train info: logloss loss:1.3440484522062364\n",
      "eval info: group_auc:0.6038, mean_mrr:0.2671, ndcg@10:0.3561, ndcg@5:0.2923\n",
      "at epoch 2 , train time: 569.7 eval time: 413.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7386it [09:29, 12.96it/s]\n",
      "85657it [06:06, 233.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 3\n",
      "train info: logloss loss:1.1953910842130349\n",
      "eval info: group_auc:0.5984, mean_mrr:0.265, ndcg@10:0.3517, ndcg@5:0.2863\n",
      "at epoch 3 , train time: 569.7 eval time: 414.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7386it [09:30, 12.96it/s]\n",
      "85657it [06:07, 233.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 4\n",
      "train info: logloss loss:1.0214095594451706\n",
      "eval info: group_auc:0.5617, mean_mrr:0.2511, ndcg@10:0.3306, ndcg@5:0.2636\n",
      "at epoch 4 , train time: 570.1 eval time: 415.2\n",
      "CPU times: total: 1h 20min 29s\n",
      "Wall time: 1h 5min 42s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<recommenders.models.newsrec.models.npa.NPAModel at 0x200f76a0ee0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Entreno\n",
    "model.fit(train_news_file, train_behaviors_file, valid_news_file, valid_behaviors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f85e4cb4-08a0-4d82-a876-a81f96a0121b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "85657it [06:07, 233.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'group_auc': 0.5617, 'mean_mrr': 0.2511, 'ndcg@5': 0.2636, 'ndcg@10': 0.3306}\n",
      "CPU times: total: 9min 31s\n",
      "Wall time: 6min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Evaluamos post entreno\n",
    "res_syn = model.run_eval(valid_news_file, valid_behaviors_file)\n",
    "print(res_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "458eb42f-8a62-4ba4-bcd8-cf0a2c06da5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el modelo\n",
    "model_path = os.path.join(data_path, \"models/npa\")\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "model.model.save_weights(os.path.join(model_path, \"npa\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b1d6c9e7-7310-4785-b49f-bfe2ae17efef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos cargados correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Leemos el modelo guardado\n",
    "epochs = 4\n",
    "seed = 42\n",
    "batch_size = 32\n",
    "\n",
    "yaml_file = os.path.join(utils_path, 'npa.yaml')\n",
    "\n",
    "# 1. Cargar hparams desde YAML\n",
    "hparams = prepare_hparams(yaml_file, \n",
    "                          wordEmb_file=wordEmb_file,\n",
    "                          wordDict_file=wordDict_file, \n",
    "                          userDict_file=userDict_file,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=epochs)\n",
    "\n",
    "iterator = MINDIterator\n",
    "\n",
    "# 2. Crear el modelo vacío\n",
    "model = NPAModel(hparams, iterator, seed=seed)\n",
    "\n",
    "# 3. Cargar los pesos\n",
    "model.model.load_weights(\"mind_small/models/npa/npa\")\n",
    "\n",
    "print(\"Pesos cargados correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2a7617a3-9c67-4734-954b-7ea06a2df1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\User\\anaconda3\\envs\\reco_env\\lib\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "85657it [07:40, 185.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'group_auc': 0.5617, 'mean_mrr': 0.2511, 'ndcg@5': 0.2636, 'ndcg@10': 0.3306}\n",
      "CPU times: total: 12min 22s\n",
      "Wall time: 8min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Evaluamos post importacion\n",
    "res_syn = model.run_eval(valid_news_file, valid_behaviors_file)\n",
    "print(res_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "178bacae-c94f-45eb-89e9-dcbbae181230",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "85657it [07:18, 195.49it/s]\n"
     ]
    }
   ],
   "source": [
    "group_impr_indexes, group_labels, group_preds = model.run_slow_eval(valid_news_file, valid_behaviors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cb1db600-0ae7-47f4-89d7-90a68b2a9ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73152it [00:00, 73873.28it/s]\n"
     ]
    }
   ],
   "source": [
    "output_file = os.path.join(data_path, 'recommendations/npa_pred_small.json')\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    for impr_index, preds in tqdm(zip(group_impr_indexes, group_preds)):\n",
    "        impr_index += 1\n",
    "\n",
    "        # Calcular el ranking\n",
    "        pred_rank = (np.argsort(np.argsort(preds)[::-1]) + 1).tolist()\n",
    "\n",
    "        # Crear estructura JSON\n",
    "        obj = {\n",
    "            \"impr_index\": int(impr_index),\n",
    "            \"pred_rank\": pred_rank\n",
    "        }\n",
    "\n",
    "        # Escribir como JSON en una línea\n",
    "        f.write(json.dumps(obj) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f676fa7b-43de-410c-81fd-7c60c998a3dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.9 (reco_env)",
   "language": "python",
   "name": "reco_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
